{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/willem/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willem/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/willem/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/willem/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "2024/07/27 23:25:23 INFO mlflow.langchain._langchain_autolog: MLflow autologging does not support logging models containing BaseRetriever because logging the model requires `loader_fn` and `persist_dir`. Please log the model manually using `mlflow.langchain.log_model(model, artifact_path, loader_fn=..., persist_dir=...)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: Ik wil een programma over het leger zien met soldaten \\nContext: Adjudant Eric Vroonen en sergeant Noémie Ghysselings werken in Kandahar als verpleger in het Green Skander veldhospitaal. In dit hightech medisch complex komen talloze Amerikaanse en Afghaanse slachtoffers binnen, en Eric en Noémie kunnen er enorm veel ervaring opdoen. Al zorgt een raketaanval soms voor ongewilde onderbrekingen. In Kunduz gaat Bas op stap met een TACP-team (Tactical Air Control Party). Er staan militaire oefeningen op het programma, want de unit moet op scherp staan. Bas mag de soldaten ook verblijden met postpakketten uit het vaderland, vooraleer de volgende missie tactisch voorbereid wordt. Een missie waarbij ook het camerateam mee op pad mag. \\nAnswer: Het is niet bekend hoeveel Amerikanese en Afghanische troepen zich bevinden bij deze hulpcentrale vanaf september 2013 tot nu toe. De situatie was echter zo hevig dat men al sindsdagen nog meer mensen aanwezig had dan ze eerder waren gebleven tijdens die periode. Dat betekent geen afstand maar wel duurzaamdheid om te bieden hun behoudensvermogen ter beschikking te hebben ten behoeve van andere militairen. En daarmee zijn zij steeds minder berechtbaar tegenover elkaars gezagvolle bevelhebber. Daarom worden hiervoor extra aangestuurd door twee personages - Sergeants Van der Lijn en Huisman – die samenwerkten voordertijd bij de Nederlandse luchtmacht. Ze nemen deel aan verschillende activiteiten zoals operaties ronduitrusting, trainingen en communiceren tussen artilleristen en piloten. Ook verzorgen ze contactpersonaalsamenwerkend met de politiekabinetschauffeur. Als eerste dienstledige werd hij ingeschreven naast drie jaar werkzaamheidsper\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, pipeline, AutoModelForCausalLM\n",
    "import transformers\n",
    "from torch import cuda, bfloat16\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# # login to hugginface (~/.cache/huggingface/token)\n",
    "# login()\n",
    "\n",
    "# Track to local MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Create a new experiment that the model and the traces will be logged to\n",
    "mlflow.set_experiment(\"StableLM Tracing\")\n",
    "\n",
    "# Enable LangChain autologging\n",
    "mlflow.langchain.autolog(log_models=True, log_input_examples=True)\n",
    "\n",
    "###############################################################################\n",
    "# Set up embeddings for later retrieval\n",
    "###############################################################################\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Postprocessing\n",
    "# Concatenate description & subtitles\n",
    "df[\"info_to_embed\"] = df[\"mediacontent_page_description\"] # + \" \" + df[\"subtitle\"]\n",
    "df = df[df.info_to_embed.notnull()]\n",
    "df= df[[\"mediacontent_page_description\",\"mediacontent_pagetitle_program\",\"info_to_embed\",\"mediacontent_pageid\"]]\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"info_to_embed\")\n",
    "catalog = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "catalog_chunks = text_splitter.split_documents(catalog)\n",
    "\n",
    "model_name = \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\"\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda:0'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "vector_store = FAISS.from_documents(documents=catalog_chunks,\n",
    "                                    embedding=hf_embedding)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Set up embeddings for later retrieval\n",
    "###############################################################################\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 1})\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "###############################################################################\n",
    "# Make Chain\n",
    "###############################################################################\n",
    "model_id= \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "        model=model_id, tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        return_full_text=True,  # langchain expects the full text(set to True when using Langchain)\n",
    "        task='text-generation', # LLM task\n",
    "        # we pass model parameters here too\n",
    "        device=0,\n",
    "        temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=256, \n",
    "    )\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Ik wil een programma over het leger zien met soldaten\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
