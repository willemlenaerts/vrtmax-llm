{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# to create a new file named vectorstore in your current directory.\n",
    "def load_knowledgeBase():\n",
    "        model_name = \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\"\n",
    "        encode_kwargs = {'normalize_embeddings': False}\n",
    "        hf = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        DB_FAISS_PATH = os.path.normpath(os.getcwd() + os.sep + os.pardir) + '/data/faiss'\n",
    "        vs = FAISS.load_local(DB_FAISS_PATH, hf,allow_dangerous_deserialization=True)\n",
    "        return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def load_prompt():\n",
    "        prompt = \"\"\" You need to answer the question in the sentence as same as in the  pdf content. . \n",
    "        Given below is the context and question of the user.\n",
    "        context = {context}\n",
    "        question = {question}\n",
    "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
    "         \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# AWS login to use sagemaker endpoints\n",
    "session = boto3.Session(profile_name='vrt-analytics-engineer-nonsensitive')\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "role = sagemaker.get_execution_role(sagemaker_session=sagemaker_session)\n",
    "client = session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the OPENAI LLM\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "\"\"\" class HFContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        self.len_prompt = len(prompt)\n",
    "        input_dict = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": model_kwargs\n",
    "        }\n",
    "        input_str = json.dumps(input_dict)\n",
    "        print(input_str)\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = output.read()\n",
    "        res = json.loads(response_json)\n",
    "        print(res)\n",
    "\n",
    "        # stripping away the input prompt from the returned response\n",
    "        ans = res[0]['generated_text'][self.len_prompt:]\n",
    "        ans = ans[:ans.rfind(\"Human\")].strip()\n",
    "        return ans \"\"\"\n",
    "\n",
    "class HFContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"messages\": [    { \"role\": \"system\", \"content\": \"You are an assistant.\" },{ \"role\": \"user\", \"content\": \"What is gold?\" }], **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json\n",
    "    \n",
    "# example parameters\n",
    "parameters = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\", # placeholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=\"huggingface-pytorch-tgi-inference-2024-07-30-20-23-30-977\",\n",
    "    client=client,\n",
    "    region_name=\"eu-west-1\",\n",
    "    model_kwargs=parameters,\n",
    "    endpoint_kwargs={\"CustomAttributes\": 'accept_eula=true'},\n",
    "    content_handler=HFContentHandler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "import streamlit as sl\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "if __name__=='__main__':\n",
    "        sl.header(\"welcome to the pdf bot\")\n",
    "        knowledgeBase=load_knowledgeBase()\n",
    "        # llm=load_llm()\n",
    "        prompt=load_prompt()\n",
    "        \n",
    "        query=sl.text_input('Enter some text')\n",
    "        \n",
    "        \n",
    "        if(query):\n",
    "                #getting only the chunks that are similar to the query for llm to produce the output\n",
    "                similar_embeddings=knowledgeBase.similarity_search(query)\n",
    "                similar_embeddings=FAISS.from_documents(documents=similar_embeddings, embedding=OpenAIEmbeddings(api_key=\"Enter your API key\"))\n",
    "                \n",
    "                #creating the chain for integrating llm,prompt,stroutputparser\n",
    "                retriever = similar_embeddings.as_retriever()\n",
    "                rag_chain = (\n",
    "                        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                        | prompt\n",
    "                        | llm\n",
    "                        | StrOutputParser()\n",
    "                    )\n",
    "                \n",
    "                response=rag_chain.invoke(query)\n",
    "                sl.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain import LLMChain\n",
    "template = \"{content}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "\n",
    "     llm=llm,\n",
    "     prompt=prompt\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "template = \"Act as an experienced but grumpy high school teacher that teaches {subject}. Always give responses in one sentence with anger.\"\n",
    "human_template = \"{text}\"\n",
    " \n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(template),\n",
    "        HumanMessage(content=\"Hello teacher!\"),\n",
    "        AIMessage(content=\"Welcome everyone!\"),\n",
    "        HumanMessagePromptTemplate.from_template(human_template),\n",
    "    ]\n",
    ")\n",
    " \n",
    "messages = chat_prompt.format_messages(\n",
    "    subject=\"Artificial Intelligence\", text=\"What is the most powerful AI model?\"\n",
    ")\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
